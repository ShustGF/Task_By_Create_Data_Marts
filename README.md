# Задание 

Необходимо создать витрину, содержащую в себе идентификаторы пользователей определенного веб-сайта.

За основу берется "Витрина A" этого сайта, партиционированная по **датам**.

Работу требуется выполнить с использованием Spark DataFrame API (Scala либо PySpark)

В результате необходимо отметить следующие шаги:
1) создать **максимально** полную витрину содержащую
в себе все возможные идентификаторы (ИНН, хэш, телефон, id внутри сайта, различные куки, GAID или IDFA);
2) сгенерировать синтетические данные для заполнения витрин;
3) алгоритм работы необходимо сделать так, чтобы его можно было запускать ежедневно для добавления
новых данный и удаления дубликататов, т.е. в витрине идентификаторов на неважно, что клиент приходит 3 дня подряд,
важно лишь зафиксировать клиента однажды(если клиент заходит с разных устройств, требуется запомнить его последние куки)

Витрины источники и примеры их заполнения можно посмотреть в файле "Витрины.docx"

Финальная витрина должна содержать следующие данные:
- user_uid
- inn
- user_phone
- user_mail
- inn_hash
- hash_phone_md5
- hash_email_md5
- org_uid
- все id из всех витрин отдельно по колонкам
- массив кук - 4 вида, в одной колонке

# Библитеки

В работе используются следующие библиотеки:
1) os - для работы с операционной системой, загрузки в данный в переменное окружение,
создания каталогов и сохранения файлов.
2) findspark и pyspark.sql - для инициализации и создания Spark сессии согласно условия задания
3) pyspark.sql.types и pyspark.sql.functions - для работы с функциями и типами данных PySpark
4) random, uuid - для генерации случайных последовательностей в данных
5) string - для сокращения кода
6) country_list и countryinfo - для информации о странах и городах. С помощью данных библиотек 
заполняются все поля связанные со местоположением 
7) hashlib - для шифрования данных в **md5** и **sha-256**
8) google.colab - для размещения сгенерированных данных в Google Collab. Данная библиотека позволит
любому желающему запустить данных код в своём кабинете Google Coolab, без нужды скачивать данный код
в локальную среду.

# Логика работы файла генерации данных "Generation_files.ipynb"

Данный файл генерирует JSON файлы согласно партиционированию по дням, в которых находятся данные. Данный код может
работать как в фоновом режиме, т.е. параллельно работать с файлом создания витрин, а можно изначально создать несколько
файлов и работать с ними. Ограничивается регистрация файлов 10ю днями(данный показатель можно увеличить в ручную). Файлы
формируются в каталогах по дням в формате **"YYYY_MM_DD"** и находятся на Google Disk по пути "/content/gdrive/MyDrive/data/json/".
Генерация данных сделана с помощью библиотек **random** и **string**. Библиотека **string** позволила получить список
буквенных(string.ascii_letters) и числовых символов(string.digits), которые с помощью метода random.choice перемешивались
случайным образом. Более подробное описание работы можно увидеть в файле "Generation_files.ipynb"

# Схема заполнения данных

Изначально была построена схема заполнения данных и из каких колонок необходимо брать нужные поля.


![Тут должна быть самый неаккуратный рисунок](https://github.com/ShustGF/task-by-create-data-marts/blob/main/Схема%20данных.png "Схема данных")


# Файл объединения витрин "Сreate_data_marts.ipynb"

На основе данных в сгенерированных JSON файлах создаются витрины данных согласно файлу "Витрины.docx", после чего все 
витрины объединяются между собой. Согласно задания создана функция "**read_file_JSON**", которая считывает данные и 
удаляет дубликаты оставляя последнее изменение в данных, т.е. последние используемые coockie пользователем. Для поиска 
данных в coockie используется функция **pyspark.sql.functions.expr**, в которую передается строка генерируемая 
функцией "search_values_from_cookie". Так же использовались встроенные в библиотеку PySpark фиункции 
хеширования данных **pyspark.sql.functions.md5** и **pyspark.sql.functions.ssh2**. Более подробное описание работы можно 
увидеть в файле "Сreate_data_marts.ipynb"



